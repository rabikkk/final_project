\documentclass[thesis]{deutez}
%%My_library 
\usepackage{placeins}%resim kaymaları için kullanıldı
\usepackage{amsmath,amsthm,mathtools}

\usepackage[breakable]{tcolorbox}
\usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)

\usepackage{iftex}
\ifPDFTeX
\usepackage[T1]{fontenc}
\usepackage{mathpazo}
\else
\usepackage{fontspec}
\fi

% Basic figure setup, for now with no caption control since it's done
% automatically by Pandoc (which extracts ![](path) syntax from Markdown).
\usepackage{graphicx}
% Maintain compatibility with old templates. Remove in nbconvert 6.0
\let\Oldincludegraphics\includegraphics
% Ensure that by default, figures have no caption (until we provide a
% proper Figure object with a Caption API and a way to capture that
% in the conversion process - todo).
%\usepackage{caption}
%\DeclareCaptionFormat{nocaption}{}
%\captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

\usepackage{float}
\floatplacement{figure}{H} % forces figures to be placed at the correct location
\usepackage{xcolor} % Allow colors to be defined
\usepackage{enumerate} % Needed for markdown enumerations to work
\usepackage{geometry} % Used to adjust the document margins
\usepackage{amsmath} % Equations
\usepackage{amssymb} % Equations
\usepackage{textcomp} % defines textquotesingle
% Hack from http://tex.stackexchange.com/a/47451/13684:
\AtBeginDocument{%
	\def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
}
\usepackage{upquote} % Upright quotes for verbatim code
\usepackage{eurosym} % defines \euro
\usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
\usepackage{fancyvrb} % verbatim replacement that allows latex
\usepackage{grffile} % extends the file name processing of package graphics 
% to support a larger range
\makeatletter % fix for old versions of grffile with XeLaTeX
\@ifpackagelater{grffile}{2019/11/01}
{
	% Do nothing on new versions
}
{
	\def\Gread@@xetex#1{%
		\IfFileExists{"\Gin@base".bb}%
		{\Gread@eps{\Gin@base.bb}}%
		{\Gread@@xetex@aux#1}%
	}
}
\makeatother
\usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
\adjustboxset{max size={1\linewidth}{1\paperheight}}

% The hyperref package gives us a pdf with properly built
% internal navigation ('pdf bookmarks' for the table of contents,
% internal cross-reference links, web links for URLs, etc.)
\usepackage{hyperref}
% The default LaTeX title has an obnoxious amount of whitespace. By default,
% titling removes some of it. It also provides customization options.
\usepackage{titling}
\usepackage{longtable} % longtable support required by pandoc >1.10
\usepackage{booktabs}  % table support for pandoc > 1.12.2
\usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
\usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
% normalem makes italics be italics, not underlines
\usepackage{mathrsfs}



% Colors for the hyperref package
%\definecolor{urlcolor}{rgb}{0,.145,.698}
%\definecolor{linkcolor}{rgb}{.71,0.21,0.01}
%\definecolor{citecolor}{rgb}{.12,.54,.11}

% ANSI colors
\definecolor{ansi-black}{HTML}{3E424D}
\definecolor{ansi-black-intense}{HTML}{282C36}
\definecolor{ansi-red}{HTML}{EA7574}
\definecolor{ansi-red-intense}{HTML}{B22B31}
\definecolor{ansi-green}{HTML}{00A250}
\definecolor{ansi-green-intense}{HTML}{007427}
\definecolor{ansi-yellow}{HTML}{DDB62B}
\definecolor{ansi-yellow-intense}{HTML}{B27D12}
\definecolor{ansi-blue}{HTML}{208FFB}
\definecolor{ansi-blue-intense}{HTML}{0065CA}
\definecolor{ansi-magenta}{HTML}{D160C4}
\definecolor{ansi-magenta-intense}{HTML}{A03196}
\definecolor{ansi-cyan}{HTML}{60C6C8}
\definecolor{ansi-cyan-intense}{HTML}{258F8F}
\definecolor{ansi-white}{HTML}{C5C1B4}
\definecolor{ansi-white-intense}{HTML}{A1A6B2}
\definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
\definecolor{ansi-default-inverse-bg}{HTML}{000000}

% common color for the border for error outputs.
\definecolor{outerrorbackground}{HTML}{FFDFDF}

% commands and environments needed by pandoc snippets
% extracted from the output of `pandoc -s`
\providecommand{\tightlist}{%
	\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}

% Additional commands for more recent versions of Pandoc
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
\newcommand{\ImportTok}[1]{{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
\newcommand{\BuiltInTok}[1]{{#1}}
\newcommand{\ExtensionTok}[1]{{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


% Define a nice break command that doesn't care if a line doesn't already
% exist.
\def\br{\hspace*{\fill} \\* }
% Math Jax compatibility definitions
\def\gt{>}
\def\lt{<}
\let\Oldtex\TeX
\let\Oldlatex\LaTeX
\renewcommand{\TeX}{\textrm{\Oldtex}}
\renewcommand{\LaTeX}{\textrm{\Oldlatex}}
% Document parameters
% Document title
\title{learn}





% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
	\let\PY@ul=\relax \let\PY@tc=\relax%
	\let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
	\PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
				\PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


% For linebreaks inside Verbatim environment from package fancyvrb. 
\makeatletter
\newbox\Wrappedcontinuationbox 
\newbox\Wrappedvisiblespacebox 
\newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
\newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
\newcommand*\Wrappedcontinuationindent {3ex } 
\newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
% Take advantage of the already applied Pygments mark-up to insert 
% potential linebreaks for TeX processing. 
%        {, <, #, %, $, ' and ": go to next line. 
%        _, }, ^, &, >, - and ~: stay at end of broken line. 
% Use of \textquotesingle for straight quote. 
\newcommand*\Wrappedbreaksatspecials {% 
	\def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
	\def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
	\def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
	\def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
	\def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
	\def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
	\def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
	\def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
	\def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
	\def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
	\def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
	\def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
	\def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
	\def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
} 
% Some characters . , ; ? ! / are not pygmentized. 
% This macro makes them "active" and they will insert potential linebreaks 
\newcommand*\Wrappedbreaksatpunct {% 
	\lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
	\lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
	\lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
	\lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
	\lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
	\lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
	\lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
	\catcode`\.\active
	\catcode`\,\active 
	\catcode`\;\active
	\catcode`\:\active
	\catcode`\?\active
	\catcode`\!\active
	\catcode`\/\active 
	\lccode`\~`\~ 	
}
\makeatother

\let\OriginalVerbatim=\Verbatim
\makeatletter
\renewcommand{\Verbatim}[1][1]{%
	%\parskip\z@skip
	\sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
	\sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
	\def\FancyVerbFormatLine ##1{\hsize\linewidth
		\vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
			\doublehyphendemerits\z@\finalhyphendemerits\z@
			\strut ##1\strut}%
	}%
	% If the linebreak is at a space, the latter will be displayed as visible
	% space at end of first line, and a continuation symbol starts next line.
	% Stretch/shrink are however usually zero for typewriter font.
	\def\FV@Space {%
		\nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
		\discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
		{\kern\fontdimen2\font}%
	}%
	
	% Allow breaks at special characters using \PYG... macros.
	\Wrappedbreaksatspecials
	% Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
	\OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
}
\makeatother

% Exact colors from NB
\definecolor{incolor}{HTML}{303F9F}
\definecolor{outcolor}{HTML}{D84315}
\definecolor{cellborder}{HTML}{CFCFCF}
\definecolor{cellbackground}{HTML}{F7F7F7}

% prompt
\makeatletter
\newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
\makeatother
\newcommand{\prompt}[4]{
	{\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
}



% Prevent overflowing lines due to hard-to-break entities
\sloppy 
% Setup hyperref package
\hypersetup{
	breaklinks=true,  % so long urls are correctly broken across lines
	colorlinks=true,
	urlcolor=black,
	linkcolor=black,
	citecolor=black,
}
% Slightly bigger margins than the latex defaults

\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}


%%Images
\graphicspath{ {/home/rabikkk/Pictures/images/} }

%-------------------------------------%%

\watermarklogo{Deu.jpg}  % Dokuz Eylül Üniversitesi Logosu; Sayfaya arkasına varsayılan logo olarak basılır.
\projectname{Smart Mirror Controller}%
\ogrencininadi{Rabia DOĞAN}%
\advisor{Ph.D. Özgür TAMER}%
\jurya{Prof. Dr. Gülay TOHUMOĞLU}
\juryb{Ph.D. Abdül BALIKCI}
\chair{Prof. Dr. Mehmet KUNTALP}
\time{January,2021}%

\begin{document}
\begin{abstract}
\paragraph~ In the project, general purpose is to give the Smart Mirror controllable features with
hand gestures. In this way, the product will be able to appeal to the upper customer
segments and increase its market share. The main goal here is to enable users to use
most of the smart mirror applications with hand movements, to eliminate the need to
touch the mirror, thus to eliminate the contamination in the touched parts of the mirrors
and therefore to possible dissatisfaction with the product and to ensure that the product
can be used even in cases where the users cannot touch the mirror with their hands for
various reasons, such as the bathroom. Although camera-based systems are generally
used for gesture recognition, it will not be welcomed by many users to have a camera in
a personal use area such as the bathroom.Therefore, passive infrared sensor arrays will
be preferred for gesture recognition in our project. They can be preferred in application
areas where cameras are relatively weak because they work with the infrared radiation
emitted by living creatures. We aim to detect hand gesture and then the movement of
the hand gesture. In this way, gesture distinction can be made with a simpler electronic
design with less processing load and lower cost than a standard camera. The project is
carried out and supported by Vestel Elektronik A.Ş. with the code TEYDEB 3170688.		
\end{abstract}
\begin{ozet}
\paragraph~ Projede genel amaç Akıllı Aynaya el jestleri ile kontrol edilebilir özellikler kazandır-
maktır.Bu sayede ürün üst müşteri segmentlerine de hitap edebilecek, pazar payını
artırabilecektir.Burada temel hedef kullanıcıların akıllı ayna uygulamalarının birçoğunu
el hareketleri ile kullanabilmesini sağlayarak, aynaya dokunma ihtiyacını ortadan kaldır-
mak böylece aynaların dokunulan bölgelerindeki kirlenmeyi ve dolayısıyla da ürün ile
ilgili olası memnuniyetsizliği ortadan kaldırmak ve banyo gibi kullanıcıların çeşitli neden-
ler ile elleri ile aynaya dokunamayacakları durumlarda dahi ürünün kullanılabilmesini
sağlamaktır.Jest tanıma için genellikle kamera temelli sistemler kullanılmakla beraber,
banyo gibi kişisel kullanıma dönük bir alanda kamera bulunması birçok kullanıcı tarafın-
dan hoş karşılanmayacaktır.Bu nedenle projemizde jest tanımlama amacıyla pasif kızılötesi
sensör dizileri tercih edilecektir.Bu tip sensörler oldukça düşük çözünürlüklerde görev
yapmasına karşın, nesne ya da canlıların yaydığı kızılötesi ışıma ile çalıştıkları için
kameraların görece zayıf kaldığı uygulama alanlarında tercih edilebilmektedirler.Pasif
kızılötesi sensörler ile öncelikle kullanıcının jestini ve sonrasında da jestin hareketini al-
gılamayı amaçlamaktayız.Bu sayede standart bir kameraya göre daha az işlem yükü ve
daha düşük maliyet ve daha basit bir elektronik tasarım ile jest ayrımı yapılabilecektir.
Proje Vestel Elektronik A.Ş.taarafından TEYDEB 3170688 kodu ile yürütülmekte ve
desteklenmektedir.

\end{ozet}
 
\tableofcontents
\listoftables
\listoffigures

\start

\chapter{INTRODUCTION}


\chapter{TECHNICAL BACKGROUND}

\section{Introduction to Convolutional Neural Networks}
\paragraph~ In deep learning, CNNs are the most common networks used with image classification. CNNs were inspired by the human visual system proposed by Fukushima\cite{neoc} and LeCun et al. \cite{lenet} State-of-the-art approaches to pattern recognition, object detection, and many other image applications. It was a deep CNN solution by Krizhevsky et al.\cite{kriz} CNNs are very different from other pattern recognition algorithms because CNNs combine both feature extraction and classification \cite{lenet}. The simple network model consists of five different layers: an input layer, a convolution layer, a pooling layer, a fully connected layer, and an output layer. These layers are divided into two parts: feature extraction and classification. Feature extraction consists of an input layer, a convolution layer, and a pool layer, while classification consists of a fully connected layer and an output layer. The input layer specifies a fixed size for input images, which are resized as needed. The image is then convoluted with multiple learned kernels using the weights shared by the convolution layer. Then, the repository layer reduces the image size while trying to preserve the information it contains. The outputs of feature extraction are known as feature maps. Classification combines extracted features into fully connected layers. Finally, there is one output neuron for each object category in the output layer. The output of the classification section is the classification result.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\textwidth]{cnn_model}
	\caption{Schematic diagram of a basic convolutional neural network (CNN) architecture\cite{phung}}
\end{figure}
\FloatBarrier
\paragraph~	Deep learning techniques have emerged recently and advances in convolutional neural networks (CNN) surpass the classical approach to hand gesture recognition as it eliminates the need to derive complex handcrafted features from images\cite{gesturecnn}. CNN's automate the feature extraction process by learning high-level abstractions in images and capturing the most distinguishing feature values using the hierarchical architecture. Thus, it solves the disadvantage of obtaining inconsistent property descriptors when working with large numbers of motion classes with very small cross-class variations\cite{static}.

\section{Model Architecture for LE-NET5}
\paragraph~ LeNet-5 was one of the earliest convolutional neural networks to support the deep learning event. After countless years of analysis and numerous compelling iterations, the final result was named LeNet-5 in 1988.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\textwidth]{lenet}
	\caption{Architecture of LeNet-5, a Convolutional Neural Network, here for digits recognition. Each plane is a feature map, i.e. a set of units
		whose weights are constrained to be identical.\cite{lenet}}
\end{figure}
\FloatBarrier

\paragraph~ LeNet-5 A total of seven layers, each with no input with trainable parameters; each layer has multiple FeatureMap, which is a property of each of the FeatureMap inputs extracted via a convolution filter, and then each FeatureMap has multiple neurons.\\

\textbf{C1 layer-convolutional layer:}\\
\paragraph~ The first convolution operation is performed on the input image (using 6 convolution kernels of size 5*5) to obtain 6 feature maps of C1 (6 feature maps of size 28 28, 32-5 + 1 = 28). The size of the convolution kernel is 5, and there are 6 (5 * 5 + 1) = 156 parameters in total, where +1 indicates that a kernel has a bias. For convolution layer C1, each pixel in C1 is dependent on 5 of 5 pixels and 1 aberration in the input image, so there are 156 28 * 28 = 122304 links in total.\cite{lenet}\\

\textbf{S2 layer-pooling layer (downsampling layer):}\\
\paragraph~ The pooling is done immediately after the first convolution. Pooling is done using 2 cores and S2, 14x14 (28/2 = 14) 6 feature maps are obtained. S2's pooling layer is a weighting coefficient plus an offset multiplied by the sum of the pixels in the 2*2 area in C1, and then the result is remapped. So each pooling core has two training parameters i.e. 2x6 = 12 training parameters but there are 5x14x14x6 = 5880 connections.\cite{lenet}\\

\textbf{C3 layer-convolutional layer:}\\
\paragraph~ After the first pooling, the second convolution, the output of the second convolution is C3, 16 pieces of 10x10 feature maps, and the size of the convolution kernel is 5. The first 6 feature maps of C3 (corresponding to column 6 of the first). red box) connects to 3 feature maps connected to S2 layer and next 6 feature maps are connected to S2 layer 4 feature maps are connected, next 3 feature maps are connected to 4 feature maps are unconnected in S2 layer and last one is linked to all feature maps in S2 layer. The convolution kernel size is still 5x5, so there are 6 (3*5*5 + 1) + 6 (4*5*5 + 1) + 3 (4*5*5 + 1) +1 (6*5*5 + 1) = 1516 parameters. The image size is 10 10 so there are 151600 connections.\cite{lenet}\\

\textbf{S4 layer-pooling layer (downsampling layer):}\\
\paragraph~ S4 is the pooling layer, the window size is still 2*2, a total of 16 feature maps and 16 10x10 maps of the C3 layer are pooled in units of 2x2 to get 16 5x5 feature maps. This layer has a total of 32 training parameters, 2x16, 5x5x5x16 = 2000 connections.\cite{lenet}\\

\textbf{C5 layer-convolution layer:}\\
\paragraph~ The C5 layer is a convolution layer. Since the size of the 16 images of the S4 layer is 5x5, the size of the image formed after convolution is 1x1, which is the same as the size of the convolution kernel. This results in 120 convolution results. Each is linked to 16 maps from the previous level. So there are (5x5x16 + 1) x120 = 48120 parameters and there are also 48120 connections.\cite{lenet}\\

\textbf{F6 layer-fully connected layer:}\\
\paragraph~ Layer 6 is a fully connected layer. The F6 layer has 84 nodes corresponding to a 7x12 bitmap, -1 means white, 1 means black, so the black and white of each symbol's bitmap corresponds to a code. The training parameters and number of connections for this layer is (120 + 1) x84 = 10164.\cite{lenet}\\
\section{Python Libraries for Project}

\subsection{Python-Peripherial}

\subsection{Tensorflow-Keras}

\chapter{MATERIALS AND METHODS}
\section{Htpa32x32d Thermopile Infrared Array}
\paragraph~ The sensor chosen to be used in the project is HTPA32x32d thermopile array sensor.
HTPA32x32d thermopile array sensor 32x32 pixel, operates between -10 and 70 degrees,
provides I2C communication, has an internal EEPROM and provides an 8-bit data set.
EEPROM data contains calibration data for each pixel of the sensor.\\

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.35\textwidth]{htpa}
	\includegraphics[width=0.5\textwidth]{htpasema}
	\caption{Schematic for HTPA32x32d}
\end{figure}
\FloatBarrier
	\begin{flushleft}
\begin{table}[h!]

	\caption{Genaral Features HTPA32x32d}
	
	
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		Features\textbackslash\{\}& Sensitivity & Therm. Pix. Time Const. & Digital Interface & EEPROM Size\\\hline
		& 450V/W&$<4ms $& I2C & 64kBit  \\\hline
		Features\textbackslash\{\}& Max Frame&Field of View & Selectable Clock & Storage Temperature \\\hline
		&60 Hz& 33*33 deg & 1 to 13 Mhz & -40/85 Deg.C \\\hline
	\end{tabular}
\end{table}
\end{flushleft}
\FloatBarrier
\section{Sensor and MCU Communication PCB Design}
\paragraph~ A mini card has been designed between the sensor and the Raspberry pi card to communicate. However, the card could not be printed. The card that will connect the sensor and the motherboard has been designed completely according to the standards recommended by Heimann company.\\
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\textwidth]{schematic}
	\caption{Schematic Design of Communication PCB}
	\includegraphics[width=0.4\textwidth]{pcb}
	\caption{PCB Design of Communication PCB}
\end{figure}
\FloatBarrier
\paragraph~ In addition to this, we have produced a product suitable for a schematic on a mini perforated plate for use in communication with the card. In addition, we designed a protective cage from a 3d printer to protect the sensor.\\
\begin{figure}[h!]
	\centering
	\includegraphics[width=1\textwidth]{mb}
	\caption{Mini Sensor Board}
\end{figure}

\section{Dataset}
\subsection{Multi-Modal Hand Gesture Dataset for Hand Gesture Recognition}
\paragraph~ This dataset was created to validate a hand-gesture recognition system for Human-Machine Interaction (HMI). It is composed of 15 different hand-gestures (4 dynamic and 11 static) that are split into 16 different hand-poses, acquired by the Leap Motion device. Hand-gestures were performed by 25 different subjects (8 women and 17 men). Every gesture has 20 instances (repetitions) per subject, performed in different locations in the image.\cite{dataset}\\
for static and dynamic gestures:\\
This set contains 16 hand-poses, used for both static and dynamic hand-gestures:\\
A: L
B: fist moved
C: index
D: ok
E: C
F: heavy
G: hang
H: two
I: three
J: four
K: five
L: palm
M: down
N: palm moved
O: palm up
P: up
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\textwidth]{data1.png}
	\caption{Hand Gesture Dataset}
\end{figure}
\FloatBarrier
\section{Data Augmentation}
\paragraph~ Using this dataset, a new train and validation set was created for the static gestures in the project. A total of 8000 and 2000 train and validation sets were created with randomly selected images.However, both resizing and data augmentation were done in order to make the data set suitable for the project.\cite{aug}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.2\textwidth]{close_test289}
	\includegraphics[width=0.2\textwidth]{index_test179}
	\includegraphics[width=0.2\textwidth]{last_test71}
	\includegraphics[width=0.2\textwidth]{open_test_0_9826}
	\caption{Close-Index-Last Page-Open Gestures}
\end{figure}
\FloatBarrier

\chapter{PROGRESS AND RESULT}
\paragraph~ In this section, the methods to be applied in the project are included.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\textwidth]{framework}
	\caption{System Framework for Static Gestures}
\end{figure}
\FloatBarrier

\section{Capture Thermopiles Image}
\subsection{Heimann Thermopile Array Sensor communication}
\paragraph~ First, I provided the connections between the sensor and the Raspberry Pi in order to receive the image. I provided the communication with the mini card I made in section 4.2 using the I2C protocol.

\paragraph~ With the device connected to a Raspberry Pi, and with the Pi configured.\cite{adaf} correctly for I2C, I was able to see the devices connected with the i2cdetect command.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\textwidth]{raspi1.png}
	\caption{Thermopile Infrared Array Device and EEPROM Addresses}
\end{figure}
\FloatBarrier

\paragraph~ In order to be able to read the data properly, the python-periphery\cite{perip} library was used. The sensor is divided into two parts (Top and Bottom Half), which are also divided into 4 blocks. The reading order is shown below for different blocks. When a conversion is initiated, the X Block of the upper and lower half are measured simultaneously. Each block consists of 128 Pixels sampled entirely in parallel. The reading order in the lower half is mirrored compared to the upper half so the center lines are always read last.

	\begin{table}[h!]
	\begin{center}
		\caption{Read Data 1 Command (Top Half of Array)}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|} 
			\hline
			Addr/CMD & \multicolumn{8}{c|}{0x1A (7 Bit!) / 0x0A}\\\cline{1-9}
			Read Data &7&6&5&4&3&2&1&0\\
			\cline{1-9}
			1. Byte / 2. Byte & \multicolumn{8}{c|}{PTAT 1 MSB / LSB or Vdd 1 MSB / LSB}\\\cline{1-9}
			3. Byte / 4. Byte & \multicolumn{8}{c|}{Pixel (0+BLOCK*128) MSB / LSB}\\\cline{1-9}
			5. Byte / 6. Byte & \multicolumn{8}{c|}{Pixel (1+BLOCK*128) MSB / LSB}\\\cline{1-9}
			... & \multicolumn{8}{c|}{}\\\cline{1-9}	
			257. Byte / 258. Byte & \multicolumn{8}{c|}{Pixel (127+BLOCK*128) MSB / LSB}\\\cline{1-9}
			
		\end{tabular}
	\end{center}
\end{table}
\FloatBarrier
\begin{table}[h!]
	\begin{center}
		\caption{Read Data 2 Command (Bottom Half of Array)}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|} 
			\hline
			Addr/CMD & \multicolumn{8}{c|}{0x1A (7 Bit!) / 0x0B}\\\cline{1-9}
			Read Data &7&6&5&4&3&2&1&0\\
			\cline{1-9}
			1. Byte / 2. Byte & \multicolumn{8}{c|}{PTAT 2 MSB / LSB or Vdd 2 MSB / LSB}\\\cline{1-9}
			3. Byte / 4. Byte & \multicolumn{8}{c|}{Pixel (992-BLOCK*128) MSB / LSB}\\\cline{1-9}
			5. Byte / 6. Byte & \multicolumn{8}{c|}{Pixel (993-BLOCK*128) MSB / LSB}\\\cline{1-9}
			... & \multicolumn{8}{c|}{}\\\cline{1-9}	
			65. Byte / 66. Byte & \multicolumn{8}{c|}{Pixel (1023-BLOCK*128) MSB / LSB}\\\cline{1-9}
			
			65. Byte / 66. Byte & \multicolumn{8}{c|}{Pixel (1023-BLOCK*128) MSB / LSB}\\\cline{1-9}
			
			67. Byte / 68. Byte & \multicolumn{8}{c|}{Pixel (960-BLOCK*128) MSB / LSB}\\\cline{1-9}
			
			69. Byte / 70. Byte & \multicolumn{8}{c|}{Pixel (961-BLOCK*128) MSB / LSB}\\\cline{1-9}
			
			... & \multicolumn{8}{c|}{}\\\cline{1-9}
			
			129. Byte / 130. Byte & \multicolumn{8}{c|}{Pixel (991-BLOCK*128) MSB / LSB}\\\cline{1-9}
			
			131. Byte / 132. Byte & \multicolumn{8}{c|}{Pixel (928-BLOCK*128) MSB / LSB}\\\cline{1-9}
			
			...& \multicolumn{8}{c|}{}\\\cline{1-9}
			
			257. Byte / 258. Bytes& \multicolumn{8}{c|}{Pixel (927-BLOCK*128) MSB / LSB}\\\cline{1-9}
			
		\end{tabular}
	\end{center}
\end{table}
\FloatBarrier
Each block is checked before it is read. The python-opencv\cite{cv2} library was used to visualize the obtained result.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.23\textwidth]{4.img}
	\includegraphics[width=0.23\textwidth]{3.img}
	\caption{Thermal Image}
\end{figure}
\FloatBarrier


\paragraph~ Each pixel (or each analog-to-digital converter, given the repeating structure corresponding to each "block" of the sensor) has its own offset and sensitivity to incident light. Without calibrating it, this constant "noise" suppresses the signal from changing IR/temperature conditions. By subtracting the two frames in quick succession, this common noise signal is removed.

\paragraph~ However, it is still quite noisy, as this frame subtraction increases random noise (since we now have contributions from two frames) and does not correct pixel-dependent sensitivity.
Only fabrication calibration will be done with EEPROM data in the next step.

\subsection{Calibrating images from Heimann Thermopile Array Sensor}

\paragraph~ After reading an image off a Heimann thermopile array, the pixel values can be converted to temperature readings through the use of calibration parameters stored on the device. To extract the calibration parameters, it is easiest to first read off the entire EEPROM on the thermopile array.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\textwidth]{eeprom}
	\caption{EEPROM overview 32x32d\cite{datasheet}}
\end{figure}
\FloatBarrier
\paragraph~ Then, parameters and calibration values can be extracted from this array, as described in the Heimann datasheet.\cite{datasheet}\\
Calibration for only one pixel is done as follows.\\
$PTAT_{av} = \frac{\sum \limits _{i=0}^{7}{PTAT_i}}{8}=38152 Digits $ \\
$PTAT_{gradient}=0.0211 dK/Digit $ and $PTAT_{offset}=2195.0dK$ \\
$ V_{00}=34435 Digits $\\
$ elOffset[0]=34240 $\\
$gradScale=24  $\\
$ThGrad_{00}=11137$ \\
$ThOffset_{00}=65506 $\\
$VDD_{av}=35000 $\\
$ VDD_{TH1}= 33942 $\\
$ VDD_{TH2} = 36942 $\\
$ PTAT_{TH1} = 30000  $\\
$ PTAT_{TH_2} = 42000  $\\
$ VddCompGrad[ 0 ] = 10356 $\\
$ VddCompOff[ 0 ] =51390  $\\
$ VddScGrad = 16 $\\
$VddScOff = 23  $\\
$ PixC_{00}= 1\cdot087\cdot10^8 $\\
$ PCSCALEVAL = 1\cdot10^8 $\\
Calculation of ambient temperature:\\
$ 	T_a=PTAT_{av} \cdot PTAT_{gradient}+ PTAT_{offset}=38152 \cdot 0.0211 + 2195.0 dK =3000 dK $\\
Compensation of thermal offset:\\
$V_{00\_Comp}=V_{00}-\frac{Th_{Grad00} \cdot T_a}{2^{gradScale}}-Th_{Offset_{00}}=34439 $\\
Compensation of electrical offset:\\
$ 	V_{00\_Comp}^{\ast} =V_{00\_Comp}-elOffset[0]=199 $\\
Compensation of supply voltage:\\
$ V_{00-VDD_{Comp}}=V_{00\_Comp}^{\ast}-\frac{\frac{Vdd_{CompGrad}[0]\cdot PTAT_{av}}{2^{Vdd_{ScGrad}}}+V_{Vdd_{Compoff}}[0]}{2^{Vdd_{ScOff}}}\\
	\cdot(VDD_{av}-VDD_{TH1}-(\frac{VDD_{TH2}-VDD_{TH1}}{PTAT_{TH2}-PTAT_{TH1}})\cdot(PTAT_{av}-PTAT_{TH1}))=199-1=198 $
The sensitivity coefficients ( PixC ij ) are calculated:\\
$PixC_{00}=(\dfrac{P_{00} \cdot (PixC_{Max}-PixC_{Min})}{65535}+PixC_{Min}) \cdot \frac{epsilon}{100} \cdot \frac{GlobalGain}{100000}=1\cdot087\cdot10^8 $\\
Leading to a compensation of the pixel voltage:\\
$V_{00PixC}=\dfrac{	V_{00-VDD_{Comp}}\cdot PCSCALEVAL}{PixC_{0}}=182$\\ 

All operations are applied for 1024 pixels. Application result images are as in figure 4.4.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.23\textwidth]{last_test58}
	\includegraphics[width=0.23\textwidth]{last_test59}
	\includegraphics[width=0.23\textwidth]{last_test60}
	\caption{Thermal Images with EEPROM Calibration Data}
\end{figure}
\FloatBarrier
\paragraph~	\textbf{NOTE:All steps to acquired the image are made with reference to the datasheet\cite{datasheet}.}
\section{Hand Thermal Image Isolation}
\paragraph~ The hand was isolated from the background without using any image processing method. For this, it has been arranged in a way that can remove the ambient temperature of the device from the image before giving a command. First, the average of 10 images was taken and given to all images. Thus, the background temperature was isolated.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.23\textwidth]{rgb0}
	\includegraphics[width=0.23\textwidth]{rgb1}
	\includegraphics[width=0.23\textwidth]{rgb2}
	\includegraphics[width=0.23\textwidth]{rgb3}
	\caption{Thermal Images with Background}
\end{figure}

\section{Hand Gesture Recognition}
\subsection{Static Gesture Recognition}


\paragraph~	A number of scenarios have been prepared for command matching for the smart mirror. The scenarios prepared are as follows;

\paragraph~	For Static Movements:

\paragraph~	There are 4 fixed movements. It is the ability to Open, Close, Tap and Return to Home.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.27\textwidth]{tek2}
	\caption{Gestures of Open,Close,Touch,Return to the Home Page }
\end{figure}
\FloatBarrier
\paragraph~ We tried to create our own data set using the data set we mentioned in Section 3.3. We used images that are similar to each of the four identified gestures. It was found correct to use K: Five set for Open, B: First Moved set for Close, C: Index set for Touch, A:L set for ,Return to the Home Page.\cite{dataset} First, the images were cropped and resized that are chosen mixed. After these processes, the data reproduced by data augmentation were divided into two as validation and training data. The number of trains defined for each movement is approximately 9200, and the number of validation images is around 2300.

\subsubsection{Model Architecture for LE-NET5}
\paragraph~ In 1989, Yann LeCun presented a convolutional neural network called LeNet. Generally, LeNet refers to LeNet-5 and is a simple convolutional neural network.\cite{lenet}

\paragraph~The fact that our Input Image sizes are 32X32 was the biggest factor pushing us to use this model. Since we could not use a ready dataset, the change in the number of layers in order to train our model well, resulted in good results.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\textwidth]{cnn}
	\caption{A~ Structural Diagram of the Proposed LENET5 for Static Gesture}
\end{figure}
\FloatBarrier
\paragraph~ The general architecture for LeNet-5 is as given in Figure 3. The input layer C1 acts like the retina, which receives centered and size-normalized gesture images (otherwise, some images may not fit in the input layer). The next layer, S2, consists of several feature maps that have the same role to gestures as their simple cells. In practice, a feature map is a square. The weights in a feature map need to be the same so they can detect the same local feature in the input image. The weights between feature maps are different so they can detect different local features. Each unit in a feature map has a receiver field.

\begin{table}[h!]
	\begin{center}
		\caption{The structural layers and number of parameters of the
			LENET5.}
\begin{tabular}{|l|l|l|}
	\hline
	\multicolumn{3}{|c|}{Model:"Static\_Gesture\_Model} \\
	\hline
	Layer(type) & Output Shape & Param\# \\
	\hline
	conv2d\_8 (Conv2D)   & (None, 28, 28, 32)  & 832 \\
	\hline
	max\_pooling2d\_8 (MaxPooling2) & (None, 14, 14, 32)   &  0 \\
	\hline
	conv2d\_9 (Conv2D)   &  (None, 10, 10, 48)    &   38448   \\
	\hline
	max\_pooling2d\_9 (MaxPooling2) & (None, 5, 5, 48)      &  0 \\
	\hline
	flatten\_4 (Flatten)  & (None, 1200)   & 0 \\
	\hline
	dense\_16 (Dense) & (None,120)  & 307456 \\
	\hline
	dense\_17 (Dense) & (None, 84) &   21588  \\
	\hline
	dense\_18 (Dense) & (None, 10) &   850  \\
	\hline
	dense\_19 (Dense)  &   (None, 4)       &   44  \\
	\hline
	\multicolumn{3}{|l|}{Total params: 369,218} \\
	\multicolumn{3}{|l|}{Trainable params: 369,218} \\
	\multicolumn{3}{|l|}{Non-trainable params: 0}\\
	\hline
\end{tabular}
	\end{center}
\end{table}
\FloatBarrier
\paragraph~ If to summarize the model we outlined in the table, we use two convolutional layers, then twice a pooling layer (32-filters and 48-filters, respectively), and finally three fully connected layers with 4-class softmax units.

\FloatBarrier
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.4\textwidth]{learning_8_0}
	\includegraphics[width=0.4\textwidth]{learning_9_0}
	\caption{Loss and Accuracy Curves (Training and Validation Set)}
\end{figure}
\FloatBarrier
\paragraph~ And the result is close to perfect. Data augmentations worked. The learning algorithm, which reached 95\% validation accuracy, gave very successful results. In the next step, images will be given to the model again for testing and the results will be observed.
\subsubsection{Test Sets and Results}
\subsection{Dynamic Gesture Recognition}

\begin{table}[h!]
	\begin{center}
		\caption{Trim Register 1 (write only)\cite{datasheet}}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
			\hline
			Addr / CMD & \multicolumn{8}{c|}{0x1A (7 Bit!) / 0x03} \\
			\hline
			Trim Reg 1 & 7 & 6 &5  &4  &3  &2  &1&0  \\
			\hline
			Name & \multicolumn{2}{c|}{RFU} & \multicolumn{2}{c|}{REF\_CAL}& \multicolumn{4}{c|}{MBIT TRIM}\\
			\hline
		\end{tabular}
	\end{center}
\end{table}
\FloatBarrier

REF\_CAL: selectable amplification
\\

MBIT\_TRIM: m = 4 to 12 (m+4) bit as ADC resolution\\

\paragraph~ In order to get the maximum efficiency from the sensor, to set the ADC Resolution to 16 bits, m of 12 was determined according to the data in the table. Since the framerate was too low, it was decided to speed up by sacrificing quality, but the quality was too low, revealing that ADC resolution should not be compromised. According to the resolutions, the images are as in figure 2.\\
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.23\textwidth]{rgb0}
	\includegraphics[width=0.23\textwidth]{rgb1}
	\includegraphics[width=0.23\textwidth]{rgb2}
	\caption{Thermal Images 8Bit-12Bit-16Bit ADC Resolution)}
\end{figure}
\begin{table}[h!]
	\begin{center}
		\caption{Trim Register 2 (write only)\cite{datasheet}}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
	\hline
	Addr / CMD & \multicolumn{8}{c|}{0x1A (7 Bit!) / 0x04} \\
	\hline
	Trim Reg 1 & 7 & 6 &5  &4  &3  &2  &1&0  \\
	\hline
	Name & \multicolumn{3}{c|}{RFU} & \multicolumn{5}{c|}{BIAS TRIM TOP}\\
	\hline
\end{tabular}
	\end{center}
\end{table}
\FloatBarrier
BIAS\_TRIM\_TOP:0 to 31 $ \longrightarrow $ $ 1\mu A $ to $ 13 \mu A $\\

\begin{table}[h!]
	\begin{center}
		\caption{Trim Register 3 (write only)\cite{datasheet}}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
			\hline
			Addr / CMD & \multicolumn{8}{c|}{0x1A (7 Bit!) / 0x05} \\
			\hline
			Trim Reg 1 & 7 & 6 &5  &4  &3  &2  &1&0  \\
			\hline
	Name & \multicolumn{3}{c|}{RFU} & \multicolumn{5}{c|}{BIAS TRIM BOT}\\
			\hline
		\end{tabular}
	\end{center}
\end{table}
\FloatBarrier
BIAS\_TRIM\_BOT: 0 to 31 $ \longrightarrow $ $ 1\mu A $ to $ 13 \mu A $\\
\paragraph~ This setting is used to adjust the bias current of the ADC. A faster clock frequency requires a
higher bias current setting.\cite{datasheet}\\

\paragraph~ Having ADC resolution 16 made us think that it did not affect us much. Afterward, it was said that if the BIAS current value is set to the maximum, its speed may increase. However, as a result of this experiment, almost no change was observed in the framerate.\\

\begin{table}[h!]
	\begin{center}
		\caption{Trim Register 4 (write only)\cite{datasheet}}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
			\hline
			Addr / CMD & \multicolumn{8}{c|}{0x1A (7 Bit!) / 0x06} \\
			\hline
			Trim Reg 1 & 7 & 6 &5  &4  &3  &2  &1&0  \\
			\hline
			Name & \multicolumn{2}{c|}{RFU} & \multicolumn{6}{c|}{CLK TRIM}\\
			\hline
		\end{tabular}
	\end{center}
\end{table}
\FloatBarrier
CLK\_TRIM:0 to 63 $ \longrightarrow $ $ 1 MHz $ to $ 13 MHz $\\

\paragraph~ The point where we could most easily observe the increase in Frame Rate was the Clock Trim set. Although I set the maximum value, I could not get the desired result here.\\
\section{Matching Gesture to Commands of Smart Mirror}

\paragraph~ In the project carried out by Vestel, friends who are interested in the smart mirror part are expected to define the gestures for the android side of the smart mirror.

\chapter{COST ANALYSIS}
\chapter{CONCLUSION}
\chapter{APPENDIX}
\input{learn}
%\includepdf[pages=-]{learn.pdf}
\bibliographystyle{deueeebst2.bst}
\bibliography{fp_refs}

\end{document}